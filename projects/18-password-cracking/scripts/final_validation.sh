#!/bin/bash
# =============================================================================
# Script de Validation Finale - Password Cracking Platform
# =============================================================================
# Ex√©cute une validation compl√®te du projet incluant :
# - Tests unitaires avec couverture de code
# - Tests d'int√©gration et s√©curit√©
# - Tests de performance et benchmarks
# - Validation de la d√©monstration compl√®te
# - G√©n√©ration de rapport de validation final
#
# Author: Cybersecurity Portfolio
# Version: 1.0.0
# Last Updated: January 2024
# =============================================================================

set -euo pipefail

# Couleurs pour output
RED='\033[0;31m'
GREEN='\033[0;32m'
YELLOW='\033[1;33m'
BLUE='\033[0;34m'
PURPLE='\033[0;35m'
CYAN='\033[0;36m'
NC='\033[0m' # No Color

# Fonctions utilitaires
log_info() { echo -e "${BLUE}[INFO]${NC} $1"; }
log_success() { echo -e "${GREEN}[SUCCESS]${NC} $1"; }
log_warning() { echo -e "${YELLOW}[WARNING]${NC} $1"; }
log_error() { echo -e "${RED}[ERROR]${NC} $1"; }
log_header() { echo -e "${PURPLE}[HEADER]${NC} $1"; }
log_step() { echo -e "${CYAN}[STEP]${NC} $1"; }

# Variables globales
PROJECT_NAME="Password Cracking Platform"
VERSION="1.0.0"
VALIDATION_REPORT="final_validation_report.md"
START_TIME=$(date +%s)
TEMP_DIR="validation_temp_$(date +%s)"

# Compteurs r√©sultats
TOTAL_TESTS=0
PASSED_TESTS=0
FAILED_TESTS=0

log_header "üöÄ VALIDATION FINALE - $PROJECT_NAME v$VERSION"
echo "================================================================================"
log_info "üìÖ D√©marrage: $(date '+%Y-%m-%d %H:%M:%S')"
log_info "üîç Environnement: $(uname -s) - $(uname -m)"
echo "================================================================================"

# 1. V√©rification pr√©requis environnement
validation_step_environment() {
    log_step "üîç √âTAPE 1/8 - V√©rification environnement..."
    
    # Python version
    if ! command -v python3 &> /dev/null; then
        log_error "Python3 n'est pas install√©"
        return 1
    fi
    
    PYTHON_VERSION=$(python3 --version 2>&1 | cut -d' ' -f2)
    log_info "Python version: $PYTHON_VERSION"
    
    # V√©rification version minimale Python 3.8
    PYTHON_MAJOR=$(echo $PYTHON_VERSION | cut -d. -f1)
    PYTHON_MINOR=$(echo $PYTHON_VERSION | cut -d. -f2)
    
    if [ "$PYTHON_MAJOR" -lt 3 ] || ([ "$PYTHON_MAJOR" -eq 3 ] && [ "$PYTHON_MINOR" -lt 8 ]); then
        log_error "Python 3.8+ requis, version actuelle: $PYTHON_VERSION"
        return 1
    fi
    
    # Pip version
    if ! command -v pip3 &> /dev/null; then
        log_error "Pip3 n'est pas install√©"
        return 1
    fi
    
    PIP_VERSION=$(pip3 --version 2>&1 | cut -d' ' -f2)
    log_info "Pip version: $PIP_VERSION"
    
    # Cr√©ation r√©pertoire temporaire
    mkdir -p "$TEMP_DIR"
    
    log_success "‚úÖ Environnement valid√©"
    return 0
}

# 2. Installation/v√©rification d√©pendances
validation_step_dependencies() {
    log_step "üì¶ √âTAPE 2/8 - Installation d√©pendances..."
    
    # V√©rification environnement virtuel
    if [ ! -d "venv" ]; then
        log_info "üîß Cr√©ation environnement virtuel..."
        python3 -m venv venv
    fi
    
    # Activation environnement virtuel
    source venv/bin/activate || {
        log_error "Impossible d'activer l'environnement virtuel"
        return 1
    }
    
    log_info "üîÑ Mise √† jour pip..."
    pip install --upgrade pip > /dev/null 2>&1
    
    # Installation d√©pendances principales
    if [ -f "requirements.txt" ]; then
        log_info "üìã Installation requirements.txt..."
        pip install -r requirements.txt > /dev/null 2>&1 || {
            log_error "√âchec installation requirements.txt"
            return 1
        }
    else
        log_error "Fichier requirements.txt manquant"
        return 1
    fi
    
    # Installation d√©pendances tests
    log_info "üß™ Installation d√©pendances tests..."
    pip install pytest pytest-cov pytest-benchmark pytest-xdist bandit safety > /dev/null 2>&1 || {
        log_warning "Certaines d√©pendances de test non install√©es"
    }
    
    log_success "‚úÖ D√©pendances install√©es"
    return 0
}

# 3. Tests unitaires avec couverture
validation_step_unit_tests() {
    log_step "üß™ √âTAPE 3/8 - Tests unitaires..."
    ((TOTAL_TESTS++))
    
    if [ ! -d "tests/unit" ]; then
        log_error "R√©pertoire tests/unit manquant"
        ((FAILED_TESTS++))
        return 1
    fi
    
    log_info "üî¨ Ex√©cution tests unitaires avec couverture..."
    
    # Ex√©cution tests avec timeout
    if timeout 300 pytest tests/unit/ -v \
        --cov=src \
        --cov-report=html:htmlcov \
        --cov-report=term \
        --cov-report=json:coverage.json \
        --cov-fail-under=85 \
        --junit-xml=unit_test_results.xml \
        -x > "$TEMP_DIR/unit_tests.log" 2>&1; then
        
        log_success "‚úÖ Tests unitaires: R√âUSSIS"
        ((PASSED_TESTS++))
        
        # Extraction m√©triques couverture
        if [ -f "coverage.json" ]; then
            COVERAGE=$(python3 -c "
import json
with open('coverage.json') as f:
    data = json.load(f)
    print(f\"{data['totals']['percent_covered']:.1f}%\")
")
            log_info "üìä Couverture de code: $COVERAGE"
        fi
        
        UNIT_TESTS_STATUS="PASSED"
        return 0
    else
        log_error "‚ùå Tests unitaires: √âCHOU√âS"
        ((FAILED_TESTS++))
        
        # Affichage des derni√®res lignes du log
        if [ -f "$TEMP_DIR/unit_tests.log" ]; then
            log_info "üìÑ Derni√®res erreurs:"
            tail -n 10 "$TEMP_DIR/unit_tests.log" | sed 's/^/   /'
        fi
        
        UNIT_TESTS_STATUS="FAILED"
        return 1
    fi
}

# 4. Tests d'int√©gration
validation_step_integration_tests() {
    log_step "üîó √âTAPE 4/8 - Tests int√©gration..."
    ((TOTAL_TESTS++))
    
    if [ ! -d "tests/integration" ]; then
        log_warning "R√©pertoire tests/integration manquant - test ignor√©"
        INTEGRATION_TESTS_STATUS="SKIPPED"
        return 0
    fi
    
    log_info "üé≠ Ex√©cution tests d'int√©gration..."
    
    if timeout 180 pytest tests/integration/ -v \
        --tb=short > "$TEMP_DIR/integration_tests.log" 2>&1; then
        
        log_success "‚úÖ Tests int√©gration: R√âUSSIS"
        ((PASSED_TESTS++))
        INTEGRATION_TESTS_STATUS="PASSED"
        return 0
    else
        log_error "‚ùå Tests int√©gration: √âCHOU√âS"
        ((FAILED_TESTS++))
        
        if [ -f "$TEMP_DIR/integration_tests.log" ]; then
            log_info "üìÑ Derni√®res erreurs:"
            tail -n 10 "$TEMP_DIR/integration_tests.log" | sed 's/^/   /'
        fi
        
        INTEGRATION_TESTS_STATUS="FAILED"
        return 1
    fi
}

# 5. Scan s√©curit√© avec Bandit
validation_step_security_scan() {
    log_step "üîí √âTAPE 5/8 - Scan s√©curit√©..."
    ((TOTAL_TESTS++))
    
    if ! command -v bandit &> /dev/null; then
        log_warning "Bandit non install√© - scan s√©curit√© ignor√©"
        SECURITY_SCAN_STATUS="SKIPPED"
        return 0
    fi
    
    log_info "üõ°Ô∏è Scan s√©curit√© Bandit..."
    
    # Ex√©cution Bandit
    if bandit -r src/ -f json -o "$TEMP_DIR/bandit_report.json" > /dev/null 2>&1; then
        # Analyse des r√©sultats
        if [ -f "$TEMP_DIR/bandit_report.json" ]; then
            # V√©rification pr√©sence de jq pour analyse JSON
            if command -v jq &> /dev/null; then
                HIGH_ISSUES=$(jq '.results | map(select(.issue_severity == "HIGH")) | length' "$TEMP_DIR/bandit_report.json" 2>/dev/null || echo "0")
                MEDIUM_ISSUES=$(jq '.results | map(select(.issue_severity == "MEDIUM")) | length' "$TEMP_DIR/bandit_report.json" 2>/dev/null || echo "0")
                
                log_info "üîç Issues trouv√©es: HIGH=$HIGH_ISSUES, MEDIUM=$MEDIUM_ISSUES"
                
                if [ "$HIGH_ISSUES" -eq 0 ]; then
                    log_success "‚úÖ Scan s√©curit√©: R√âUSSI (0 vuln√©rabilit√© haute)"
                    ((PASSED_TESTS++))
                    SECURITY_SCAN_STATUS="PASSED"
                    return 0
                else
                    log_error "‚ùå Scan s√©curit√©: √âCHOU√â ($HIGH_ISSUES vuln√©rabilit√©s hautes)"
                    ((FAILED_TESTS++))
                    SECURITY_SCAN_STATUS="FAILED"
                    return 1
                fi
            else
                log_warning "jq non disponible - analyse des r√©sultats limit√©e"
                log_success "‚úÖ Scan s√©curit√©: TERMIN√â (v√©rification manuelle requise)"
                ((PASSED_TESTS++))
                SECURITY_SCAN_STATUS="WARNING"
                return 0
            fi
        fi
    else
        # Bandit peut retourner un code d'erreur m√™me avec juste des warnings
        log_warning "‚ö†Ô∏è Scan s√©curit√©: WARNINGS d√©tect√©s"
        SECURITY_SCAN_STATUS="WARNING"
        return 0
    fi
}

# 6. Tests performance
validation_step_performance_tests() {
    log_step "‚ö° √âTAPE 6/8 - Tests performance..."
    ((TOTAL_TESTS++))
    
    if [ ! -d "tests/performance" ]; then
        log_warning "R√©pertoire tests/performance manquant - tests ignor√©s"
        PERFORMANCE_TESTS_STATUS="SKIPPED"
        return 0
    fi
    
    log_info "üìà Ex√©cution tests performance et benchmarks..."
    
    if timeout 240 pytest tests/performance/ -v \
        --benchmark-only \
        --benchmark-json="$TEMP_DIR/benchmark_results.json" \
        > "$TEMP_DIR/performance_tests.log" 2>&1; then
        
        log_success "‚úÖ Tests performance: R√âUSSIS"
        ((PASSED_TESTS++))
        
        # Affichage m√©triques si disponibles
        if [ -f "$TEMP_DIR/benchmark_results.json" ]; then
            if command -v jq &> /dev/null; then
                BENCHMARK_COUNT=$(jq '.benchmarks | length' "$TEMP_DIR/benchmark_results.json" 2>/dev/null || echo "N/A")
                log_info "üìä Benchmarks ex√©cut√©s: $BENCHMARK_COUNT"
            fi
        fi
        
        PERFORMANCE_TESTS_STATUS="PASSED"
        return 0
    else
        log_error "‚ùå Tests performance: √âCHOU√âS"
        ((FAILED_TESTS++))
        
        if [ -f "$TEMP_DIR/performance_tests.log" ]; then
            log_info "üìÑ Derni√®res erreurs:"
            tail -n 5 "$TEMP_DIR/performance_tests.log" | sed 's/^/   /'
        fi
        
        PERFORMANCE_TESTS_STATUS="FAILED"
        return 1
    fi
}

# 7. Validation d√©monstration compl√®te
validation_step_demo_validation() {
    log_step "üé≠ √âTAPE 7/8 - Validation d√©monstration..."
    ((TOTAL_TESTS++))
    
    if [ ! -f "examples/complete_audit_demo.py" ]; then
        log_error "Fichier examples/complete_audit_demo.py manquant"
        ((FAILED_TESTS++))
        DEMO_VALIDATION_STATUS="FAILED"
        return 1
    fi
    
    log_info "üé™ Ex√©cution d√©monstration compl√®te..."
    
    # Ex√©cution d√©mo avec timeout de 5 minutes
    if timeout 300 python3 examples/complete_audit_demo.py > "$TEMP_DIR/demo_output.log" 2>&1; then
        log_success "‚úÖ D√©monstration: R√âUSSIE"
        ((PASSED_TESTS++))
        
        # V√©rification fichiers de sortie
        DEMO_DIRS=$(find examples/ -type d -name "demo_*" 2>/dev/null | wc -l)
        if [ "$DEMO_DIRS" -gt 0 ]; then
            log_info "üìÅ R√©pertoires d√©mo cr√©√©s: $DEMO_DIRS"
        fi
        
        DEMO_VALIDATION_STATUS="PASSED"
        return 0
    else
        log_error "‚ùå D√©monstration: √âCHOU√âE ou timeout"
        ((FAILED_TESTS++))
        
        if [ -f "$TEMP_DIR/demo_output.log" ]; then
            log_info "üìÑ Sortie d√©monstration:"
            tail -n 15 "$TEMP_DIR/demo_output.log" | sed 's/^/   /'
        fi
        
        DEMO_VALIDATION_STATUS="FAILED"
        return 1
    fi
}

# 8. V√©rification structure projet
validation_step_project_structure() {
    log_step "üìÅ √âTAPE 8/8 - V√©rification structure..."
    ((TOTAL_TESTS++))
    
    # Fichiers critiques requis
    REQUIRED_FILES=(
        "src/analysis/password_analyzer.py"
        "src/wordlist_generator/wordlist_builder.py"
        "docs/user_guide.md"
        "docs/testing_validation_guide.md"
        "tests/unit/test_password_analyzer.py"
        "examples/complete_audit_demo.py"
        "requirements.txt"
        "README.md"
    )
    
    # R√©pertoires critiques requis
    REQUIRED_DIRS=(
        "src/analysis"
        "src/wordlist_generator"
        "tests/unit"
        "docs"
        "examples"
    )
    
    MISSING_FILES=0
    MISSING_DIRS=0
    
    log_info "üîç V√©rification fichiers critiques..."
    for file in "${REQUIRED_FILES[@]}"; do
        if [ ! -f "$file" ]; then
            log_error "üìÑ Fichier manquant: $file"
            ((MISSING_FILES++))
        fi
    done
    
    log_info "üîç V√©rification r√©pertoires critiques..."
    for dir in "${REQUIRED_DIRS[@]}"; do
        if [ ! -d "$dir" ]; then
            log_error "üìÅ R√©pertoire manquant: $dir"
            ((MISSING_DIRS++))
        fi
    done
    
    if [ $MISSING_FILES -eq 0 ] && [ $MISSING_DIRS -eq 0 ]; then
        log_success "‚úÖ Structure projet: COMPL√àTE"
        ((PASSED_TESTS++))
        PROJECT_STRUCTURE_STATUS="COMPLETE"
        return 0
    else
        log_error "‚ùå Structure projet: INCOMPL√àTE ($MISSING_FILES fichiers, $MISSING_DIRS r√©pertoires manquants)"
        ((FAILED_TESTS++))
        PROJECT_STRUCTURE_STATUS="INCOMPLETE"
        return 1
    fi
}

# G√©n√©ration rapport final
generate_final_report() {
    log_step "üìä G√©n√©ration rapport final..."
    
    # Calcul temps total
    END_TIME=$(date +%s)
    TOTAL_TIME=$((END_TIME - START_TIME))
    
    # Calcul pourcentage succ√®s
    if [ $TOTAL_TESTS -gt 0 ]; then
        SUCCESS_RATE=$((PASSED_TESTS * 100 / TOTAL_TESTS))
    else
        SUCCESS_RATE=0
    fi
    
    # D√©termination statut global
    GLOBAL_STATUS="FAILED"
    GLOBAL_ICON="‚ùå"
    
    if [ "$UNIT_TESTS_STATUS" == "PASSED" ] && 
       [ "$PROJECT_STRUCTURE_STATUS" == "COMPLETE" ] && 
       [ "$DEMO_VALIDATION_STATUS" == "PASSED" ]; then
        
        GLOBAL_STATUS="PASSED"
        GLOBAL_ICON="‚úÖ"
        
        # V√©rification optionnels
        if [ "$SECURITY_SCAN_STATUS" != "PASSED" ] || 
           [ "$INTEGRATION_TESTS_STATUS" != "PASSED" ] || 
           [ "$PERFORMANCE_TESTS_STATUS" != "PASSED" ]; then
            GLOBAL_STATUS="PASSED_WITH_WARNINGS"
            GLOBAL_ICON="‚ö†Ô∏è"
        fi
    fi
    
    # G√©n√©ration rapport Markdown
    cat > "$VALIDATION_REPORT" << EOF
# üéØ Rapport de Validation Finale
## $PROJECT_NAME v$VERSION

**Date:** $(date '+%Y-%m-%d %H:%M:%S')  
**Dur√©e totale:** ${TOTAL_TIME}s  
**Environnement:** $(python3 --version 2>&1), $(uname -s)  
**Statut global:** $GLOBAL_ICON **$GLOBAL_STATUS**

---

## üìä R√©sum√© des R√©sultats

| **M√©trique** | **Valeur** |
|--------------|------------|
| Tests ex√©cut√©s | $TOTAL_TESTS |
| Tests r√©ussis | $PASSED_TESTS |
| Tests √©chou√©s | $FAILED_TESTS |
| Taux de succ√®s | $SUCCESS_RATE% |
| Couverture code | ${COVERAGE:-"N/A"} |

---

## üìã D√©tail des Validations

| **Cat√©gorie** | **Statut** | **Description** |
|---------------|------------|-----------------|
| Tests Unitaires | **${UNIT_TESTS_STATUS:-"NOT_RUN"}** | Suite compl√®te de tests avec couverture |
| Tests Int√©gration | **${INTEGRATION_TESTS_STATUS:-"NOT_RUN"}** | Workflow et int√©gration des composants |
| Scan S√©curit√© | **${SECURITY_SCAN_STATUS:-"NOT_RUN"}** | Analyse statique Bandit |
| Tests Performance | **${PERFORMANCE_TESTS_STATUS:-"NOT_RUN"}** | Benchmarks et tests de charge |
| D√©monstration | **${DEMO_VALIDATION_STATUS:-"NOT_RUN"}** | Validation audit complet end-to-end |
| Structure Projet | **${PROJECT_STRUCTURE_STATUS:-"NOT_RUN"}** | V√©rification fichiers et r√©pertoires |

---

## üéØ Conclusion

EOF

    case $GLOBAL_STATUS in
        "PASSED")
            cat >> "$VALIDATION_REPORT" << EOF
**üéâ VALIDATION COMPL√àTEMENT R√âUSSIE**

Toutes les validations critiques sont pass√©es avec succ√®s. La plateforme est **pr√™te pour utilisation en production**.

### ‚úÖ Points forts identifi√©s :
- Architecture robuste et bien test√©e
- Couverture de tests excellente (${COVERAGE:-">85%"})
- S√©curit√© valid√©e sans vuln√©rabilit√©s critiques
- D√©monstration fonctionnelle compl√®te
- Documentation compl√®te et √† jour

### üöÄ Recommandations de d√©ploiement :
- La plateforme peut √™tre d√©ploy√©e en production
- Surveillance continue recommand√©e
- Formation utilisateurs conseill√©e

EOF
            ;;
        "PASSED_WITH_WARNINGS")
            cat >> "$VALIDATION_REPORT" << EOF
**‚ö†Ô∏è VALIDATION R√âUSSIE AVEC AVERTISSEMENTS**

Les validations critiques sont pass√©es, mais certains tests optionnels ont √©chou√© ou √©t√© ignor√©s.

### ‚úÖ √âl√©ments valid√©s :
- Fonctionnalit√©s principales op√©rationnelles
- Tests unitaires r√©ussis
- D√©monstration fonctionnelle

### ‚ö†Ô∏è Points d'attention :
- Tests d'int√©gration : ${INTEGRATION_TESTS_STATUS:-"Non ex√©cut√©s"}
- Scan s√©curit√© : ${SECURITY_SCAN_STATUS:-"Non ex√©cut√©"}
- Tests performance : ${PERFORMANCE_TESTS_STATUS:-"Non ex√©cut√©s"}

### üìù Actions recommand√©es :
- Corriger les tests en √©chec avant production
- Compl√©ter la suite de tests manquante
- R√©vision s√©curit√© suppl√©mentaire conseill√©e

EOF
            ;;
        "FAILED")
            cat >> "$VALIDATION_REPORT" << EOF
**‚ùå VALIDATION √âCHOU√âE**

Des validations critiques ont √©chou√©. La plateforme **n'est pas pr√™te** pour utilisation en production.

### üö® Probl√®mes critiques identifi√©s :
EOF
            
            [ "$UNIT_TESTS_STATUS" == "FAILED" ] && echo "- Tests unitaires en √©chec" >> "$VALIDATION_REPORT"
            [ "$DEMO_VALIDATION_STATUS" == "FAILED" ] && echo "- D√©monstration non fonctionnelle" >> "$VALIDATION_REPORT"
            [ "$PROJECT_STRUCTURE_STATUS" == "INCOMPLETE" ] && echo "- Structure projet incompl√®te" >> "$VALIDATION_REPORT"
            
            cat >> "$VALIDATION_REPORT" << EOF

### üîß Actions requises :
1. **PRIORIT√â HAUTE** : Corriger tous les tests unitaires
2. **PRIORIT√â HAUTE** : Valider la d√©monstration compl√®te
3. **PRIORIT√â MOYENNE** : Compl√©ter la structure du projet
4. **PRIORIT√â MOYENNE** : R√©viser les tests d'int√©gration

### ‚è≥ Prochaines √©tapes :
- Corriger les probl√®mes identifi√©s
- Relancer la validation compl√®te
- R√©vision code recommand√©e

EOF
            ;;
    esac

    cat >> "$VALIDATION_REPORT" << EOF

---

## üìÅ Fichiers G√©n√©r√©s

- **Rapport principal** : \`$VALIDATION_REPORT\`
- **Logs d√©taill√©s** : \`$TEMP_DIR/\`
- **Rapport couverture** : \`htmlcov/index.html\` (si g√©n√©r√©)
- **R√©sultats Bandit** : \`$TEMP_DIR/bandit_report.json\` (si g√©n√©r√©)

---

*Rapport g√©n√©r√© automatiquement par scripts/final_validation.sh*  
*$PROJECT_NAME v$VERSION - $(date '+%Y-%m-%d %H:%M:%S')*
EOF

    log_success "üìÑ Rapport g√©n√©r√©: $VALIDATION_REPORT"
}

# Fonction de nettoyage
cleanup() {
    log_info "üßπ Nettoyage en cours..."
    
    # Conservation des logs importants
    if [ -d "$TEMP_DIR" ]; then
        if [ "$GLOBAL_STATUS" == "FAILED" ]; then
            log_info "üì¶ Conservation des logs d'erreur dans: $TEMP_DIR"
        else
            rm -rf "$TEMP_DIR" 2>/dev/null || true
        fi
    fi
    
    # D√©sactivation environnement virtuel
    deactivate 2>/dev/null || true
}

# Fonction principale
main() {
    # Configuration trap pour nettoyage
    trap cleanup EXIT INT TERM
    
    # Initialisation variables statut
    UNIT_TESTS_STATUS="NOT_RUN"
    INTEGRATION_TESTS_STATUS="NOT_RUN"
    SECURITY_SCAN_STATUS="NOT_RUN"
    PERFORMANCE_TESTS_STATUS="NOT_RUN"
    DEMO_VALIDATION_STATUS="NOT_RUN"
    PROJECT_STRUCTURE_STATUS="NOT_RUN"
    COVERAGE="N/A"
    
    # Ex√©cution s√©quentielle des validations
    validation_step_environment || {
        log_error "üí• √âchec validation environnement"
        generate_final_report
        exit 1
    }
    
    validation_step_dependencies || {
        log_error "üí• √âchec installation d√©pendances"
        generate_final_report
        exit 1
    }
    
    # Les autres validations continuent m√™me en cas d'√©chec
    validation_step_unit_tests
    validation_step_integration_tests
    validation_step_security_scan
    validation_step_performance_tests
    validation_step_demo_validation
    validation_step_project_structure
    
    # G√©n√©ration rapport final
    generate_final_report
    
    # Affichage r√©sum√© final
    echo ""
    log_header "üèÅ VALIDATION TERMIN√âE"
    echo "================================================================================"
    log_info "‚è±Ô∏è Dur√©e totale: $(($(date +%s) - START_TIME))s"
    log_info "üìä Tests: $PASSED_TESTS/$TOTAL_TESTS r√©ussis ($SUCCESS_RATE%)"
    log_info "üìÑ Rapport: $VALIDATION_REPORT"
    echo "================================================================================"
    
    case $GLOBAL_STATUS in
        "PASSED")
            log_success "üéâ VALIDATION COMPL√àTEMENT R√âUSSIE!"
            log_success "üöÄ Plateforme pr√™te pour production"
            exit 0
            ;;
        "PASSED_WITH_WARNINGS")
            log_warning "‚ö†Ô∏è VALIDATION R√âUSSIE AVEC AVERTISSEMENTS"
            log_warning "üìù R√©vision recommand√©e avant production"
            exit 0
            ;;
        *)
            log_error "üí• VALIDATION √âCHOU√âE"
            log_error "üîß Corrections requises avant utilisation"
            exit 1
            ;;
    esac
}

# Point d'entr√©e
if [[ "${BASH_SOURCE[0]}" == "${0}" ]]; then
    main "$@"
fi