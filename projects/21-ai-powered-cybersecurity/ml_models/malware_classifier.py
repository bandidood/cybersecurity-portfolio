#!/usr/bin/env python3
"""
Malware Classification Model
Advanced ensemble-based malware family classification and detection
Author: AI Cybersecurity Team
Version: 1.0.0
"""

import numpy as np
import pandas as pd
import hashlib
import pefile
import yara
import magic
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, VotingClassifier
from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score
from sklearn.decomposition import PCA
from sklearn.cluster import KMeans
import xgboost as xgb
import lightgbm as lgb
from typing import Dict, List, Tuple, Any, Optional, Union
import pickle
import joblib
import logging
import warnings
import os
import re
import json
from pathlib import Path

warnings.filterwarnings('ignore')

# Configure logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class MalwareClassifier:
    """
    Advanced Malware Classification System using multiple ML techniques:
    - Static analysis feature extraction from PE files
    - Dynamic behavior analysis
    - String and opcode analysis
    - Ensemble classification with Random Forest, XGBoost, and LightGBM
    - Family clustering and attribution
    """
    
    def __init__(self, config: Dict[str, Any] = None):
        """Initialize the Malware Classifier"""
        self.config = config or self._default_config()
        
        # Initialize classifiers
        self.rf_classifier = RandomForestClassifier(
            n_estimators=self.config['rf_n_estimators'],
            max_depth=self.config['rf_max_depth'],
            random_state=42,
            n_jobs=-1
        )
        
        self.xgb_classifier = xgb.XGBClassifier(
            n_estimators=self.config['xgb_n_estimators'],
            max_depth=self.config['xgb_max_depth'],
            learning_rate=self.config['xgb_learning_rate'],
            random_state=42,
            eval_metric='mlogloss'
        )
        
        self.lgb_classifier = lgb.LGBMClassifier(
            n_estimators=self.config['lgb_n_estimators'],
            max_depth=self.config['lgb_max_depth'],
            learning_rate=self.config['lgb_learning_rate'],
            random_state=42,
            verbose=-1
        )
        
        # Ensemble classifier
        self.ensemble_classifier = None
        
        # Preprocessing components
        self.scaler = StandardScaler()
        self.label_encoder = LabelEncoder()
        self.tfidf_vectorizer = TfidfVectorizer(
            max_features=self.config['tfidf_max_features'],
            ngram_range=(1, 3)
        )
        self.pca = PCA(n_components=self.config['pca_components'])
        
        # Feature extraction components
        self.yara_rules = None
        self.string_patterns = []
        self.api_calls_vocab = []
        
        # Model storage
        self.feature_names = []
        self.family_names = []
        self.is_trained = False
        
    def _default_config(self) -> Dict[str, Any]:
        """Default configuration for the classifier"""
        return {
            # Random Forest parameters
            'rf_n_estimators': 200,
            'rf_max_depth': 20,
            
            # XGBoost parameters
            'xgb_n_estimators': 200,
            'xgb_max_depth': 10,
            'xgb_learning_rate': 0.1,
            
            # LightGBM parameters
            'lgb_n_estimators': 200,
            'lgb_max_depth': 10,
            'lgb_learning_rate': 0.1,
            
            # Feature extraction parameters
            'tfidf_max_features': 5000,
            'pca_components': 100,
            'min_string_length': 4,
            'max_api_calls': 1000,
            
            # Ensemble parameters
            'ensemble_voting': 'soft',
            'cv_folds': 5,
            
            # Data generation parameters
            'n_samples_per_family': 1000,
            'noise_level': 0.1
        }
    
    def extract_pe_features(self, file_path: str) -> Dict[str, Any]:
        """
        Extract static features from PE files
        
        Args:
            file_path: Path to PE file
            
        Returns:
            Dictionary of extracted features
        """
        features = {}
        
        try:
            # Basic file information
            file_size = os.path.getsize(file_path)
            features['file_size'] = file_size
            
            # Calculate file hashes
            with open(file_path, 'rb') as f:
                file_content = f.read()
                features['md5_hash'] = hashlib.md5(file_content).hexdigest()
                features['sha1_hash'] = hashlib.sha1(file_content).hexdigest()
                features['sha256_hash'] = hashlib.sha256(file_content).hexdigest()
            
            # File type detection
            try:
                file_type = magic.from_file(file_path)
                features['file_type'] = file_type
                features['is_pe'] = 'PE32' in file_type
            except:
                features['file_type'] = 'unknown'
                features['is_pe'] = False
            
            # PE file analysis
            if features['is_pe']:
                try:
                    pe = pefile.PE(file_path)
                    
                    # PE header information
                    features['pe_timestamp'] = pe.FILE_HEADER.TimeDateStamp
                    features['pe_machine_type'] = pe.FILE_HEADER.Machine
                    features['pe_num_sections'] = pe.FILE_HEADER.NumberOfSections
                    features['pe_characteristics'] = pe.FILE_HEADER.Characteristics
                    
                    # Optional header information
                    if hasattr(pe, 'OPTIONAL_HEADER'):
                        features['pe_entry_point'] = pe.OPTIONAL_HEADER.AddressOfEntryPoint
                        features['pe_image_base'] = pe.OPTIONAL_HEADER.ImageBase
                        features['pe_size_of_image'] = pe.OPTIONAL_HEADER.SizeOfImage
                        features['pe_size_of_headers'] = pe.OPTIONAL_HEADER.SizeOfHeaders
                        features['pe_subsystem'] = pe.OPTIONAL_HEADER.Subsystem
                        features['pe_dll_characteristics'] = pe.OPTIONAL_HEADER.DllCharacteristics
                    
                    # Section information
                    section_names = []
                    total_raw_size = 0
                    total_virtual_size = 0
                    executable_sections = 0
                    
                    for section in pe.sections:
                        section_name = section.Name.decode('utf-8', errors='ignore').rstrip('\x00')
                        section_names.append(section_name)
                        total_raw_size += section.SizeOfRawData
                        total_virtual_size += section.Misc_VirtualSize
                        
                        # Check if section is executable
                        if section.Characteristics & 0x20000000:
                            executable_sections += 1
                    
                    features['pe_section_names'] = ','.join(section_names)
                    features['pe_total_raw_size'] = total_raw_size
                    features['pe_total_virtual_size'] = total_virtual_size
                    features['pe_executable_sections'] = executable_sections
                    
                    # Import table analysis
                    imported_dlls = []
                    imported_functions = []
                    
                    if hasattr(pe, 'DIRECTORY_ENTRY_IMPORT'):
                        for entry in pe.DIRECTORY_ENTRY_IMPORT:
                            dll_name = entry.dll.decode('utf-8', errors='ignore')
                            imported_dlls.append(dll_name)
                            
                            for imp in entry.imports:
                                if imp.name:
                                    func_name = imp.name.decode('utf-8', errors='ignore')
                                    imported_functions.append(func_name)
                    
                    features['pe_imported_dlls'] = ','.join(imported_dlls[:50])  # Limit size
                    features['pe_num_imported_dlls'] = len(imported_dlls)
                    features['pe_num_imported_functions'] = len(imported_functions)
                    
                    # Export table analysis
                    exported_functions = []
                    if hasattr(pe, 'DIRECTORY_ENTRY_EXPORT'):
                        for exp in pe.DIRECTORY_ENTRY_EXPORT.symbols:
                            if exp.name:
                                func_name = exp.name.decode('utf-8', errors='ignore')
                                exported_functions.append(func_name)
                    
                    features['pe_num_exported_functions'] = len(exported_functions)
                    
                    # Resource analysis
                    if hasattr(pe, 'DIRECTORY_ENTRY_RESOURCE'):
                        features['pe_has_resources'] = True
                        features['pe_num_resources'] = len(pe.DIRECTORY_ENTRY_RESOURCE.entries)
                    else:
                        features['pe_has_resources'] = False
                        features['pe_num_resources'] = 0
                    
                    pe.close()
                    
                except Exception as e:
                    logger.warning(f"Error analyzing PE file {file_path}: {e}")
                    # Set default values for PE features
                    for key in ['pe_timestamp', 'pe_machine_type', 'pe_num_sections', 
                               'pe_entry_point', 'pe_image_base', 'pe_num_imported_dlls']:
                        features[key] = 0
            
            # String extraction
            strings = self.extract_strings(file_content)
            features['num_strings'] = len(strings)
            features['avg_string_length'] = np.mean([len(s) for s in strings]) if strings else 0
            features['max_string_length'] = max([len(s) for s in strings]) if strings else 0
            
            # Entropy calculation
            features['entropy'] = self.calculate_entropy(file_content)
            
        except Exception as e:
            logger.error(f"Error extracting features from {file_path}: {e}")
            # Return minimal features
            features = {
                'file_size': 0,
                'is_pe': False,
                'num_strings': 0,
                'entropy': 0
            }
        
        return features
    
    def extract_strings(self, data: bytes, min_length: int = None) -> List[str]:
        """Extract printable strings from binary data"""
        if min_length is None:
            min_length = self.config['min_string_length']
        
        # ASCII strings
        ascii_strings = re.findall(rb'[\x20-\x7e]{%d,}' % min_length, data)
        ascii_strings = [s.decode('ascii', errors='ignore') for s in ascii_strings]
        
        # Unicode strings
        unicode_strings = re.findall(rb'(?:[\x20-\x7e]\x00){%d,}' % min_length, data)
        unicode_strings = [s.decode('utf-16le', errors='ignore') for s in unicode_strings]
        
        return ascii_strings + unicode_strings
    
    def calculate_entropy(self, data: bytes) -> float:
        """Calculate Shannon entropy of binary data"""
        if not data:
            return 0.0
        
        # Count byte frequencies
        byte_counts = np.bincount(np.frombuffer(data, dtype=np.uint8))
        probabilities = byte_counts[byte_counts > 0] / len(data)
        
        # Calculate entropy
        entropy = -np.sum(probabilities * np.log2(probabilities))
        return entropy
    
    def generate_synthetic_malware_data(self, n_samples: int = 5000) -> pd.DataFrame:
        """
        Generate synthetic malware data for demonstration
        
        Args:
            n_samples: Total number of samples to generate
            
        Returns:
            DataFrame with synthetic malware features and labels
        """
        np.random.seed(42)
        
        # Define malware families and their characteristics
        families = {
            'Trojan.Win32.Agent': {
                'file_size_mean': 50000, 'file_size_std': 20000,
                'entropy_mean': 7.2, 'entropy_std': 0.5,
                'num_sections_mean': 4, 'num_sections_std': 1,
                'num_imports_mean': 50, 'num_imports_std': 20,
                'packed_prob': 0.3, 'has_resources_prob': 0.8
            },
            'Worm.Win32.Conficker': {
                'file_size_mean': 150000, 'file_size_std': 50000,
                'entropy_mean': 6.8, 'entropy_std': 0.3,
                'num_sections_mean': 6, 'num_sections_std': 2,
                'num_imports_mean': 80, 'num_imports_std': 30,
                'packed_prob': 0.7, 'has_resources_prob': 0.9
            },
            'Backdoor.Win32.Poison': {
                'file_size_mean': 80000, 'file_size_std': 30000,
                'entropy_mean': 7.8, 'entropy_std': 0.4,
                'num_sections_mean': 5, 'num_sections_std': 1,
                'num_imports_mean': 60, 'num_imports_std': 25,
                'packed_prob': 0.8, 'has_resources_prob': 0.6
            },
            'Virus.Win32.Sality': {
                'file_size_mean': 30000, 'file_size_std': 15000,
                'entropy_mean': 6.5, 'entropy_std': 0.6,
                'num_sections_mean': 3, 'num_sections_std': 1,
                'num_imports_mean': 30, 'num_imports_std': 15,
                'packed_prob': 0.2, 'has_resources_prob': 0.4
            },
            'Rootkit.Win32.ZeroAccess': {
                'file_size_mean': 200000, 'file_size_std': 80000,
                'entropy_mean': 7.5, 'entropy_std': 0.3,
                'num_sections_mean': 8, 'num_sections_std': 2,
                'num_imports_mean': 100, 'num_imports_std': 40,
                'packed_prob': 0.9, 'has_resources_prob': 0.7
            },
            'Adware.Win32.Gator': {
                'file_size_mean': 120000, 'file_size_std': 40000,
                'entropy_mean': 6.2, 'entropy_std': 0.4,
                'num_sections_mean': 5, 'num_sections_std': 2,
                'num_imports_mean': 70, 'num_imports_std': 25,
                'packed_prob': 0.4, 'has_resources_prob': 0.9
            },
            'Spyware.Win32.Keylogger': {
                'file_size_mean': 60000, 'file_size_std': 25000,
                'entropy_mean': 7.0, 'entropy_std': 0.5,
                'num_sections_mean': 4, 'num_sections_std': 1,
                'num_imports_mean': 45, 'num_imports_std': 20,
                'packed_prob': 0.5, 'has_resources_prob': 0.3
            },
            'Ransomware.Win32.WannaCry': {
                'file_size_mean': 300000, 'file_size_std': 100000,
                'entropy_mean': 7.9, 'entropy_std': 0.2,
                'num_sections_mean': 7, 'num_sections_std': 2,
                'num_imports_mean': 90, 'num_imports_std': 35,
                'packed_prob': 0.8, 'has_resources_prob': 0.8
            }
        }
        
        samples_per_family = n_samples // len(families)
        all_samples = []
        
        for family_name, characteristics in families.items():
            family_samples = []
            
            for _ in range(samples_per_family):
                sample = {}
                
                # Generate basic features based on family characteristics
                sample['file_size'] = max(1000, int(np.random.normal(
                    characteristics['file_size_mean'], 
                    characteristics['file_size_std']
                )))
                
                sample['entropy'] = np.clip(np.random.normal(
                    characteristics['entropy_mean'], 
                    characteristics['entropy_std']
                ), 0, 8)
                
                sample['pe_num_sections'] = max(1, int(np.random.normal(
                    characteristics['num_sections_mean'],
                    characteristics['num_sections_std']
                )))
                
                sample['pe_num_imported_functions'] = max(0, int(np.random.normal(
                    characteristics['num_imports_mean'],
                    characteristics['num_imports_std']
                )))
                
                # Binary features
                sample['is_packed'] = np.random.random() < characteristics['packed_prob']
                sample['pe_has_resources'] = np.random.random() < characteristics['has_resources_prob']
                
                # PE characteristics
                sample['pe_entry_point'] = np.random.randint(0x1000, 0x10000)
                sample['pe_image_base'] = 0x400000  # Common base address
                sample['pe_timestamp'] = np.random.randint(1000000000, 1600000000)  # Unix timestamp
                sample['pe_machine_type'] = 332  # i386
                
                # String features
                sample['num_strings'] = np.random.poisson(100)
                sample['avg_string_length'] = np.random.exponential(10)
                sample['max_string_length'] = np.random.exponential(50)
                
                # Import/Export features
                sample['pe_num_imported_dlls'] = np.random.poisson(10)
                sample['pe_num_exported_functions'] = np.random.poisson(5)
                sample['pe_executable_sections'] = np.random.randint(1, sample['pe_num_sections'] + 1)
                
                # Resource features
                if sample['pe_has_resources']:
                    sample['pe_num_resources'] = np.random.poisson(5)
                else:
                    sample['pe_num_resources'] = 0
                
                # Size features
                sample['pe_total_raw_size'] = int(sample['file_size'] * np.random.uniform(0.8, 1.0))
                sample['pe_total_virtual_size'] = int(sample['pe_total_raw_size'] * np.random.uniform(1.0, 1.5))
                sample['pe_size_of_headers'] = np.random.randint(1024, 4096)
                
                # System features
                sample['pe_subsystem'] = np.random.choice([2, 3])  # GUI or Console
                sample['pe_dll_characteristics'] = np.random.randint(0, 0x8000)
                sample['pe_characteristics'] = np.random.randint(0, 0x8000)
                
                # Label
                sample['family'] = family_name
                
                family_samples.append(sample)
            
            all_samples.extend(family_samples)
        
        # Convert to DataFrame
        df = pd.DataFrame(all_samples)
        
        # Add some noise to make it more realistic
        numeric_columns = df.select_dtypes(include=[np.number]).columns
        for col in numeric_columns:
            if col != 'family':
                noise = np.random.normal(0, df[col].std() * self.config['noise_level'], len(df))
                df[col] += noise
        
        # Shuffle the data
        df = df.sample(frac=1).reset_index(drop=True)
        
        logger.info(f"Generated {len(df)} synthetic malware samples")
        logger.info(f"Family distribution:")
        for family, count in df['family'].value_counts().items():
            logger.info(f"  {family}: {count} samples")
        
        return df
    
    def preprocess_features(self, df: pd.DataFrame, fit: bool = False) -> np.ndarray:
        """
        Preprocess features for machine learning
        
        Args:
            df: DataFrame with extracted features
            fit: Whether to fit preprocessing components
            
        Returns:
            Preprocessed feature matrix
        """
        logger.info("Preprocessing malware features...")
        
        # Select numeric features
        numeric_features = df.select_dtypes(include=[np.number]).columns
        if 'family' in numeric_features:
            numeric_features = numeric_features.drop('family')
        
        # Handle missing values
        df_processed = df[numeric_features].fillna(0)
        
        # Store feature names
        if fit:
            self.feature_names = numeric_features.tolist()
        
        # Convert to numpy array
        feature_matrix = df_processed.values.astype(np.float32)
        
        # Scale features
        if fit:
            feature_matrix = self.scaler.fit_transform(feature_matrix)
        else:
            feature_matrix = self.scaler.transform(feature_matrix)
        
        return feature_matrix
    
    def fit(self, X: np.ndarray, y: np.ndarray) -> Dict[str, Any]:
        """
        Train the malware classification models
        
        Args:
            X: Feature matrix
            y: Labels
            
        Returns:
            Training metrics
        """
        logger.info("Training malware classification models...")
        
        # Encode labels
        y_encoded = self.label_encoder.fit_transform(y)
        self.family_names = self.label_encoder.classes_.tolist()
        
        # Train individual classifiers
        logger.info("Training Random Forest...")
        self.rf_classifier.fit(X, y_encoded)
        
        logger.info("Training XGBoost...")
        self.xgb_classifier.fit(X, y_encoded)
        
        logger.info("Training LightGBM...")
        self.lgb_classifier.fit(X, y_encoded)
        
        # Create ensemble classifier
        self.ensemble_classifier = VotingClassifier(
            estimators=[
                ('rf', self.rf_classifier),
                ('xgb', self.xgb_classifier),
                ('lgb', self.lgb_classifier)
            ],
            voting=self.config['ensemble_voting']
        )
        
        logger.info("Training ensemble classifier...")
        self.ensemble_classifier.fit(X, y_encoded)
        
        # Calculate training metrics
        rf_score = cross_val_score(self.rf_classifier, X, y_encoded, 
                                  cv=self.config['cv_folds'], scoring='accuracy')
        xgb_score = cross_val_score(self.xgb_classifier, X, y_encoded, 
                                   cv=self.config['cv_folds'], scoring='accuracy')
        lgb_score = cross_val_score(self.lgb_classifier, X, y_encoded, 
                                   cv=self.config['cv_folds'], scoring='accuracy')
        ensemble_score = cross_val_score(self.ensemble_classifier, X, y_encoded, 
                                        cv=self.config['cv_folds'], scoring='accuracy')
        
        training_metrics = {
            'random_forest_cv_accuracy': np.mean(rf_score),
            'random_forest_cv_std': np.std(rf_score),
            'xgboost_cv_accuracy': np.mean(xgb_score),
            'xgboost_cv_std': np.std(xgb_score),
            'lightgbm_cv_accuracy': np.mean(lgb_score),
            'lightgbm_cv_std': np.std(lgb_score),
            'ensemble_cv_accuracy': np.mean(ensemble_score),
            'ensemble_cv_std': np.std(ensemble_score),
            'num_features': X.shape[1],
            'num_classes': len(self.family_names),
            'training_samples': X.shape[0]
        }
        
        self.is_trained = True
        logger.info("Training completed successfully")
        
        return training_metrics
    
    def predict(self, X: np.ndarray, return_proba: bool = False) -> Union[np.ndarray, Tuple[np.ndarray, np.ndarray]]:
        """
        Predict malware families
        
        Args:
            X: Feature matrix
            return_proba: Whether to return prediction probabilities
            
        Returns:
            Predictions and optionally probabilities
        """
        if not self.is_trained:
            raise ValueError("Model must be trained before making predictions")
        
        # Get ensemble predictions
        predictions = self.ensemble_classifier.predict(X)
        predicted_families = self.label_encoder.inverse_transform(predictions)
        
        if return_proba:
            probabilities = self.ensemble_classifier.predict_proba(X)
            return predicted_families, probabilities
        else:
            return predicted_families
    
    def predict_individual(self, X: np.ndarray) -> Dict[str, np.ndarray]:
        """
        Get predictions from individual classifiers
        
        Args:
            X: Feature matrix
            
        Returns:
            Dictionary with predictions from each classifier
        """
        if not self.is_trained:
            raise ValueError("Model must be trained before making predictions")
        
        # Get predictions from each classifier
        rf_pred = self.label_encoder.inverse_transform(self.rf_classifier.predict(X))
        xgb_pred = self.label_encoder.inverse_transform(self.xgb_classifier.predict(X))
        lgb_pred = self.label_encoder.inverse_transform(self.lgb_classifier.predict(X))
        ensemble_pred = self.label_encoder.inverse_transform(self.ensemble_classifier.predict(X))
        
        return {
            'random_forest': rf_pred,
            'xgboost': xgb_pred,
            'lightgbm': lgb_pred,
            'ensemble': ensemble_pred
        }
    
    def get_feature_importance(self) -> Dict[str, np.ndarray]:
        """
        Get feature importance from trained models
        
        Returns:
            Dictionary with feature importance from each model
        """
        if not self.is_trained:
            raise ValueError("Model must be trained before getting feature importance")
        
        importance_dict = {}
        
        # Random Forest feature importance
        importance_dict['random_forest'] = self.rf_classifier.feature_importances_
        
        # XGBoost feature importance
        importance_dict['xgboost'] = self.xgb_classifier.feature_importances_
        
        # LightGBM feature importance
        importance_dict['lightgbm'] = self.lgb_classifier.feature_importances_
        
        # Average importance across models
        importance_dict['average'] = np.mean([
            importance_dict['random_forest'],
            importance_dict['xgboost'],
            importance_dict['lightgbm']
        ], axis=0)
        
        return importance_dict
    
    def evaluate(self, X: np.ndarray, y_true: np.ndarray) -> Dict[str, Any]:
        """
        Evaluate model performance
        
        Args:
            X: Test features
            y_true: True labels
            
        Returns:
            Evaluation metrics
        """
        if not self.is_trained:
            raise ValueError("Model must be trained before evaluation")
        
        # Get predictions from all models
        predictions = self.predict_individual(X)
        
        results = {}
        
        for model_name, y_pred in predictions.items():
            accuracy = accuracy_score(y_true, y_pred)
            
            results[model_name] = {
                'accuracy': accuracy,
                'classification_report': classification_report(y_true, y_pred, output_dict=True)
            }
        
        return results
    
    def save_model(self, model_path: str):
        """Save trained model"""
        if not self.is_trained:
            raise ValueError("Model must be trained before saving")
        
        model_data = {
            'rf_classifier': self.rf_classifier,
            'xgb_classifier': self.xgb_classifier,
            'lgb_classifier': self.lgb_classifier,
            'ensemble_classifier': self.ensemble_classifier,
            'scaler': self.scaler,
            'label_encoder': self.label_encoder,
            'feature_names': self.feature_names,
            'family_names': self.family_names,
            'config': self.config
        }
        
        joblib.dump(model_data, f"{model_path}.pkl")
        logger.info(f"Model saved to {model_path}.pkl")
    
    def load_model(self, model_path: str):
        """Load trained model"""
        model_data = joblib.load(f"{model_path}.pkl")
        
        self.rf_classifier = model_data['rf_classifier']
        self.xgb_classifier = model_data['xgb_classifier']
        self.lgb_classifier = model_data['lgb_classifier']
        self.ensemble_classifier = model_data['ensemble_classifier']
        self.scaler = model_data['scaler']
        self.label_encoder = model_data['label_encoder']
        self.feature_names = model_data['feature_names']
        self.family_names = model_data['family_names']
        self.config.update(model_data['config'])
        self.is_trained = True
        
        logger.info(f"Model loaded from {model_path}.pkl")

def main():
    """Demonstration of Malware Classification"""
    logger.info("Starting Malware Classification demonstration...")
    
    # Initialize classifier
    classifier = MalwareClassifier()
    
    # Generate synthetic data
    data = classifier.generate_synthetic_malware_data(n_samples=8000)
    
    # Separate features and labels
    X = classifier.preprocess_features(data.drop('family', axis=1), fit=True)
    y = data['family'].values
    
    # Split data
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.3, stratify=y, random_state=42
    )
    
    # Train models
    logger.info("Training malware classification models...")
    training_metrics = classifier.fit(X_train, y_train)
    
    # Evaluate models
    logger.info("Evaluating models...")
    evaluation_results = classifier.evaluate(X_test, y_test)
    
    # Print results
    print("\n" + "="*60)
    print("MALWARE CLASSIFICATION RESULTS")
    print("="*60)
    
    print("\nTraining Metrics:")
    for metric, value in training_metrics.items():
        print(f"  {metric}: {value:.4f}")
    
    print("\nEvaluation Results:")
    for model, metrics in evaluation_results.items():
        print(f"\n{model.upper()} Model:")
        print(f"  Accuracy: {metrics['accuracy']:.4f}")
        
        # Print per-class metrics
        report = metrics['classification_report']
        print("  Per-class metrics:")
        for family in classifier.family_names:
            if family in report:
                precision = report[family]['precision']
                recall = report[family]['recall']
                f1 = report[family]['f1-score']
                print(f"    {family}: P={precision:.3f}, R={recall:.3f}, F1={f1:.3f}")
    
    # Feature importance analysis
    importance = classifier.get_feature_importance()
    print("\nTop 10 Most Important Features (Average):")
    feature_importance_pairs = list(zip(classifier.feature_names, importance['average']))
    feature_importance_pairs.sort(key=lambda x: x[1], reverse=True)
    
    for i, (feature_name, importance_value) in enumerate(feature_importance_pairs[:10]):
        print(f"  {i+1}. {feature_name}: {importance_value:.4f}")
    
    # Save model
    model_path = "projects/21-ai-powered-cybersecurity/ml_models/malware_classifier_model"
    classifier.save_model(model_path)
    
    logger.info("Malware Classification demonstration completed!")

if __name__ == "__main__":
    main()